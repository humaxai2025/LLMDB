# LLM DB ü§ñ

**The Ultimate Large Language Model Comparison Tool**

LLM DB is a comprehensive, user-friendly web application that helps developers, researchers, and AI enthusiasts compare and choose the right Large Language Model (LLM) for their needs. With 150 carefully curated production models from 39+ providers, you'll never struggle to find pricing, context windows, or API integration details again.

[![Made with ‚ù§Ô∏è](https://img.shields.io/badge/Made%20with-%E2%9D%A4%EF%B8%8F-red)](https://github.com)
[![Next.js](https://img.shields.io/badge/Next.js-15-black)](https://nextjs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5-blue)](https://www.typescriptlang.org/)
[![Tailwind CSS](https://img.shields.io/badge/Tailwind-3-38bdf8)](https://tailwindcss.com/)

---

## üéØ Why LLM DB?

Choosing the right LLM can be overwhelming. Different providers, different pricing models, varying context windows, and scattered documentation make it hard to make informed decisions. **LLM DB solves this by putting all the information you need in one place.**

### The Problem
- ü§Ø **Too many models**: OpenAI, Anthropic, Google, Meta, Mistral, and 39+ other providers
- üí∞ **Complex pricing**: Per-token costs, input vs output pricing, varying by provider
- üìä **Hard to compare**: Context windows range from 4K to 2M tokens
- üîç **Scattered info**: Official docs across dozens of websites
- üíª **Integration confusion**: Different SDKs, APIs, and authentication methods
- üîÑ **Outdated information**: Models and pricing change frequently

### The Solution
LLM DB gives you:
- ‚úÖ **Instant comparison** of 150 production models in a sortable table
- ‚úÖ **Real-time cost calculator** to estimate your expenses
- ‚úÖ **Accurate token counting** with real-time cost estimation
- ‚úÖ **Usage analytics** to track your LLM spending and patterns
- ‚úÖ **Quality scores** based on benchmark performance
- ‚úÖ **Python code samples** ready to copy-paste for every model
- ‚úÖ **Advanced filtering** by price, context size, provider, status, and more
- ‚úÖ **Side-by-side comparison** of up to 4 models
- ‚úÖ **Export functionality** from any view (CSV, JSON, Markdown)
- ‚úÖ **API availability info** with endpoints, authentication, and rate limits
- ‚úÖ **Status indicators** showing NEW, UPDATED, and DEPRECATED models

---

## ‚ú® Key Features

### üíª Practical Implementation Features

#### Working Code Examples
- **1,200+ code samples** (8 per model)
- **Python examples**: Basic chat, streaming responses, function calling
- **JavaScript examples**: Async/await, streaming, error handling
- **cURL examples**: Basic requests, streaming
- **Syntax highlighting** with dark mode support
- **One-click copy** to clipboard
- **Illustrative examples** with clear disclaimers

#### Real-World Use Cases
- **600+ industry scenarios** (3-5 per model)
- **7 industry verticals**: Legal, Finance, Healthcare, E-commerce, Software, Education, Media
- **Metrics included**: Time saved, accuracy, cost per operation, ROI
- **Difficulty ratings**: Easy, Medium, Hard
- **Implementation details** and real results
- **Note**: Illustrative examples based on common use patterns

#### Limitations & Best Practices
- **Comprehensive guides** for all 150 models
- **6 categories**: Known issues, common failures, content policies, performance, best practices, workarounds
- **Provider-specific warnings**: Rate limits, authentication, regional restrictions
- **Universal limitations**: Hallucinations, knowledge cutoff, consistency
- **Expert tips**: Error handling, cost optimization, production deployment
- **Official docs references** for verification

### üìä Comprehensive Model Database
- **150 production models** - carefully curated, no fake/placeholder models
- **39+ providers** including OpenAI, Anthropic, Google, Meta, xAI, DeepSeek, and more
- **100% API coverage** - every model includes complete API information
- **Status indicators** - NEW (2024-2025 releases), UPDATED (recent pricing changes), DEPRECATED
- Regular updates with latest models and pricing

### üîç Advanced Search & Filtering
- **Multi-criteria search** - filter by provider, model type, capabilities, and features
- **Status filters** - find new models, recently updated pricing, or deprecated models
- **Quick filters** - cheapest, largest context, best value, your favorites
- **Real-time search** - instant results as you type
- **Smart sorting** - by price, context window, quality score, or release date

### üí∞ Cost Analysis Tools
- **Pricing per million tokens** for both input and output
- **Real-world cost examples** (1K tokens, 10K tokens, 100K tokens)
- **Cost calculator** to estimate your monthly expenses
- **Sort by price** to find the most economical option
- **Books capacity calculator** - understand context in terms of books (100K tokens each)

### üß† Smart Model Comparison
- **Side-by-side comparison** of up to 4 models simultaneously
- **Quality scores** calculated from MMLU and HumanEval benchmarks
- **Context window size** - see which models can handle long documents
- **Performance benchmarks** (MMLU, HumanEval, inference speed)
- **Best use cases** - recommended applications for each model
- **Visual comparison** - easily spot differences across models

### üíª Developer-Friendly Integration üÜï
- **1,200+ code examples** - 8 per model (Python, JavaScript, cURL)
- **3 languages supported** - Python, JavaScript/Node.js, and cURL
- **Multiple use cases** - Basic requests, streaming, function calling, error handling
- **Copy-to-clipboard** - One-click code copying with syntax highlighting
- **Installation commands** - Ready pip/npm install commands
- **API integration examples** - Proper authentication for all providers
- **Dark mode code blocks** - Professional syntax highlighting

### üåê API Availability Information
- **Complete endpoint URLs** - ready to use in your code
- **Authentication methods** - API keys, tokens, OAuth details
- **Rate limits** - understand tier-based quotas and throttling
- **Regional availability** - know where each model is accessible
- **Official documentation links** - quick access to provider docs
- **100% coverage** - every model includes complete API information

### üìä Usage Analytics & Tracking
- **Enhanced Token Counter** - Accurate token counting using GPT tokenizer
  - Real-time token counting as you type
  - Separate tracking for system, user, and assistant messages
  - Context window usage visualization with color-coded warnings
  - Per-request cost estimation with input/output breakdown
  - Model-specific pricing and context limits
- **Session History** - Automatic tracking of your LLM usage
  - Stores up to 1,000 sessions in your browser (100% private)
  - Time-based filtering (today, week, month, all-time)
  - Comprehensive statistics dashboard
  - Track total costs, tokens, and identify usage patterns
  - Model and provider usage distribution
  - One-click export of usage data
- **Smart Model Selection** - Choose any models for analysis
  - Search through all 150+ models
  - Add/remove models dynamically
  - Quick clear button to reset selections
  - No arbitrary limits

### üì• Universal Export Functionality
- **Export from Anywhere** - Available in all major views
  - Model Details view - export single model data
  - Comparison view - export compared models
  - Cost Calculator - export calculation results
  - Analytics - comprehensive reports
- **Multiple Formats**
  - CSV - Excel/Google Sheets compatible
  - JSON - For developers and automation
  - Markdown - Documentation-ready tables
- **Smart File Naming** - Auto-generated with timestamps
- **One-Click Download** - No complex steps required

### üé® Beautiful User Interface
- **Collapsible sections** - only see what you need
- **Dark mode** support with automatic theme switching
- **Responsive design** - works perfectly on mobile, tablet, and desktop
- **Keyboard shortcuts** for power users
- **Favorites** - star your most-used models
- **Search & filter** - find models instantly

### üì± Enhanced Model Detail View
Click any model to see:
- **Export Button** - Download model data in CSV, JSON, or Markdown format
- **Status Badges** - NEW, UPDATED, or DEPRECATED indicators in header
- **Overview** - primary purpose, best use cases, key features
- **Pricing & Economics** - detailed cost breakdown with examples
- **Technical Specs** - context window, capacity, last updated
- **Performance Benchmarks** - MMLU, HumanEval, speed ratings
- **API Availability** - endpoints, authentication, rate limits, regional availability
- **Working Code Examples** - Python, JavaScript, and cURL samples (8 per model = 1,200+ total)
- **Real-World Use Cases** - Industry-specific examples with metrics (600+ scenarios)
- **Limitations & Best Practices** - Known issues, gotchas, and expert tips
- **Tags** - quick identification of model capabilities

---

## üöÄ Quick Start

### For Users
Simply visit the live site and start exploring! No installation needed.

### For Developers

**Prerequisites:**
- Node.js 18+ installed
- npm or yarn package manager

**Installation:**

```bash
# Clone the repository
git clone <your-repo-url>
cd LLMDB

# Install dependencies
npm install

# Run development server
npm run dev

# Open http://localhost:3000
```

**Build for production:**

```bash
npm run build
npm start
```

---

## üìñ How to Use

### 1Ô∏è‚É£ Browse Models
- View the comprehensive table of all LLM models
- Sort by **name**, **provider**, **context window**, **price**, or **quality score**
- Use the search bar to find specific models
- **Status badges** show NEW, UPDATED, or DEPRECATED models directly in the table

### 2Ô∏è‚É£ Advanced Search & Filter
- Click **"Advanced Search"** button for multi-criteria filtering:
  - **Providers** - filter by specific providers (OpenAI, Anthropic, Google, etc.)
  - **Model Types** - chat, completion, embedding, multimodal
  - **Capabilities** - vision, function calling, streaming, etc.
  - **Status** - find new models, recently updated pricing, or deprecated ones
  - **Features** - long context, reasoning, coding specialist, etc.
- Click **quick filter tabs** to view:
  - All models
  - Cheapest options
  - Large context models
  - Best value models
  - Your favorites
- **Star models** to add them to favorites
- **Select up to 4 models** to compare side-by-side

### 3Ô∏è‚É£ View Details
- Click the **‚ÑπÔ∏è info icon** or **View Details** button on any model
- Click **green Export button** to download model data (CSV/JSON/Markdown)
- See **status badges** (NEW/UPDATED/DEPRECATED) in the model header
- Explore organized, collapsible sections:
  - **Overview** - what the model is good for
  - **Pricing & Economics** - detailed cost breakdown
  - **Context & Performance** - technical specifications
  - **Best For** - recommended use cases
  - **Key Features** - unique capabilities
  - **API Integration** - endpoints, authentication, rate limits, docs
  - **Working Code Examples** - Python, JavaScript, cURL (expand to see)
  - **Real-World Use Cases** - Industry examples with metrics (expand to see)
  - **Limitations & Best Practices** - Gotchas and expert tips (expand to see)
  - **Tags** - model capabilities
- **Copy code** with one click
- **Expand/collapse** sections for easier navigation

### 4Ô∏è‚É£ Track Usage & Costs
- Click **"Analytics"** button (gradient blue-purple in header)
- **Token Counter Tab**:
  - Enter system instructions, user prompts, and expected responses
  - See real-time token counts with accurate GPT tokenization
  - View context window usage with color-coded warnings (green/yellow/red)
  - Get instant cost estimates for each request
- **Session History Tab**:
  - View all your token calculations automatically saved
  - Filter by time (today, week, month, all-time)
  - See comprehensive statistics (total cost, tokens, most-used model)
  - Track usage patterns and spending trends
  - Export your usage data
- **Export & Reports Tab**:
  - Export selected models in CSV, JSON, or Markdown
  - Generate comprehensive usage reports
  - Download session history data
- **Model Selection**:
  - Click "Add/Remove Models" to select any models from database
  - Search through 150+ models by name or provider
  - Clear button to quickly reset selections

### 5Ô∏è‚É£ Calculate Costs
- Click **"Calculator"** button
- Add models for cost calculation
- Enter your expected token usage
- See estimated costs for selected models
- Click **green Export button** to download calculation results

### 6Ô∏è‚É£ Compare Models
- Select up to 4 models for comparison
- Click **"Compare"** button
- View side-by-side comparison with benchmarks and costs
- Click **green Export button** to download comparison data

### 7Ô∏è‚É£ Keyboard Shortcuts
Press **?** to see all keyboard shortcuts:
- `/` - Focus search
- `c` - Toggle calculator
- `?` - Show keyboard help

---

## üèóÔ∏è Tech Stack

| Technology | Purpose |
|------------|---------|
| **Next.js 15** | React framework with App Router |
| **TypeScript** | Type-safe development |
| **Tailwind CSS** | Utility-first styling |
| **Lucide React** | Beautiful icons |
| **Vercel** | Deployment platform |

---

## üé® Features in Detail

### Cost Calculator
Estimate your monthly LLM costs based on:
- Expected monthly token usage
- Input/output token ratio
- Model pricing
- Real-time calculations

### Quality Scoring
We calculate quality scores using:
- **MMLU (Massive Multitask Language Understanding)** - 60% weight
- **HumanEval (Coding Capability)** - 40% weight
- Normalized to 0-10 scale
- Color-coded badges (green = excellent, blue = good, etc.)

### Code Examples
Every model includes **8 working code samples**:

**Python (3 examples):**
- Basic Chat Completion - Simple request with system/user messages
- Streaming Response - Real-time output streaming
- Function Calling - Tool use and function invocation

**JavaScript (3 examples):**
- Basic Chat Completion - Async/await patterns
- Streaming Response - For-await streaming
- Error Handling - Production-ready error management

**cURL (2 examples):**
- Basic Request - Command-line testing
- Streaming Request - Server-sent events

All examples include:
- **Provider-specific SDKs** (OpenAI, Anthropic, Google, etc.)
- **Correct authentication** patterns
- **Working imports** and initialization
- **Syntax highlighting** with dark mode
- **Copy-to-clipboard** functionality

### Supported Providers
OpenAI ‚Ä¢ Anthropic ‚Ä¢ Google ‚Ä¢ Meta ‚Ä¢ Mistral ‚Ä¢ Cohere ‚Ä¢ AWS ‚Ä¢ Azure ‚Ä¢ xAI ‚Ä¢ DeepSeek ‚Ä¢ Perplexity ‚Ä¢ Together AI ‚Ä¢ Groq ‚Ä¢ Fireworks ‚Ä¢ Anyscale ‚Ä¢ Replicate ‚Ä¢ Alibaba ‚Ä¢ Baidu ‚Ä¢ Tencent ‚Ä¢ Zhipu ‚Ä¢ MiniMax ‚Ä¢ Moonshot ‚Ä¢ AI21 ‚Ä¢ Databricks ‚Ä¢ Nvidia ‚Ä¢ Reka ‚Ä¢ Hugging Face ‚Ä¢ Stability AI ‚Ä¢ Microsoft ‚Ä¢ Amazon ‚Ä¢ Intel ‚Ä¢ IBM ‚Ä¢ and many more...

---

## üõ†Ô∏è Customization

### Adding New Models

Edit `app/data/llm-data.ts`:

```typescript
{
  id: "model-id",
  name: "Model Name",
  provider: "Provider Name",
  contextWindow: 128000,
  inputCostPer1M: 10.00,
  outputCostPer1M: 30.00,
  releaseDate: "2024-11-08",
  description: "Model description",
  strengths: ["Complex tasks", "Long documents", "High performance"],
  apiInfo: {
    endpoint: "https://api.provider.com/v1/chat/completions",
    authentication: "API Key (Bearer token)",
    rateLimits: "10,000 TPM on tier 1",
    regionalAvailability: "Global",
    documentation: "https://docs.provider.com/api"
  },
  status: {
    isNew: true,
    pricingUpdated: true,
    pricingUpdateDate: "2024-11-08"
  }
}
```

### Updating Pricing

Pricing is in `app/data/llm-data.ts`. Update `inputCostPer1M` and `outputCostPer1M` fields. When updating pricing, also update the status badge:

```typescript
status: {
  pricingUpdated: true,
  pricingUpdateDate: "2025-11-08"  // Today's date
}
```

### Styling

Customize in:
- `tailwind.config.ts` - Colors, spacing, fonts
- `app/globals.css` - Global styles and CSS variables

---

## üì¶ Project Structure

```
LLMDB/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ [modelId]/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx                        # Model detail page
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ModelDetailsCard.tsx            # Enhanced model card with Phase 1 features
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Phase1Features.tsx              # üÜï Code examples, use cases, limitations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MobileModelCard.tsx             # Mobile-responsive card view
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AdvancedSearch.tsx              # Multi-criteria search & filtering
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ EnhancedModelComparison.tsx     # Side-by-side model comparison (up to 4)
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm-data.ts                     # All model data (150 models)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ phase1-data-generator.ts        # üÜï Dynamic Phase 1 content generation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ enriched-models.ts              # üÜï Models with Phase 1 features
‚îÇ   ‚îú‚îÄ‚îÄ types/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ features.ts                     # TypeScript types
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ modelRecommendations.ts         # Smart model suggestions
‚îÇ   ‚îú‚îÄ‚îÄ globals.css                         # Global styles
‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx                          # Root layout
‚îÇ   ‚îî‚îÄ‚îÄ page.tsx                            # Main table view with mobile support
‚îú‚îÄ‚îÄ components/                              # Additional components
‚îú‚îÄ‚îÄ public/                                  # Static assets
‚îú‚îÄ‚îÄ next.config.mjs                         # Next.js config
‚îú‚îÄ‚îÄ tailwind.config.ts                      # Tailwind config
‚îî‚îÄ‚îÄ package.json                            # Dependencies
```

---

## üöÄ Deployment

### Deploy to Vercel (Recommended)

**Option 1: Via GitHub**
1. Push code to GitHub
2. Visit [vercel.com](https://vercel.com)
3. Click "Import Project"
4. Select your repository
5. Click "Deploy"

**Option 2: Via CLI**
```bash
npm install -g vercel
vercel
```

### Deploy to Other Platforms
- **Netlify**: Connect GitHub repo and deploy
- **Railway**: Import from GitHub
- **Self-hosted**: Run `npm run build` and serve `.next` folder

---

## üåü Use Cases

### For Developers
- Compare API pricing before choosing a provider
- Get working Python code instantly
- Calculate monthly costs based on usage
- Find models with specific context windows

### For Researchers
- Compare benchmark scores across models
- Identify best models for specific tasks
- Track latest model releases
- Understand capability differences

### For Businesses
- Estimate LLM infrastructure costs
- Compare quality vs price trade-offs
- Find most cost-effective options
- Plan for scale with context window info

### For Students
- Learn about different LLM providers
- Understand pricing models
- Access Python integration examples
- Explore latest AI models

---

## üéØ Roadmap

### ‚úÖ Recently Completed
- [x] **Enhanced Token Counter**: Accurate tokenization with real-time cost estimation
- [x] **Session History & Analytics**: Track usage, costs, and patterns over time
- [x] **Universal Export**: Export from any view (Model Details, Comparison, Calculator, Analytics)
- [x] **Smart Model Selection**: Search and select from all 150+ models with quick clear option
- [x] **1,200+ Code Examples**: Python, JavaScript, and cURL samples for all models
- [x] **600+ Use Cases**: Real-world industry scenarios with metrics
- [x] **150 Limitation Guides**: Comprehensive best practices and gotchas
- [x] **Mobile Responsive Design**: Perfect UX on all devices
- [x] **Collapsible Sections**: Clean, organized modal dialogs
- [x] API Availability Information (endpoints, auth, rate limits)
- [x] Model Status Indicators (NEW, UPDATED, DEPRECATED badges)
- [x] Advanced Search & Filtering (multi-criteria)
- [x] Enhanced Model Comparison (up to 4 models)
- [x] Database cleanup (150 curated production models)

### üìã Planned Features
- [ ] **Live Testing Playground**: Test models directly in-browser
- [ ] **User Reviews & Ratings**: Community-driven insights
- [ ] **Community Prompts Library**: Share and discover effective prompts
- [ ] **Smart Model Selector Wizard**: Answer questions to find perfect model
- [ ] **Real News Feed**: Latest model updates and changes
- [ ] API cost comparison charts
- [ ] Save custom comparisons to browser storage
- [ ] Email/webhook alerts for price changes
- [ ] Model changelog tracking
- [ ] Usage calculator with custom workloads
- [ ] Provider status monitoring

---

## ü§ù Contributing

Contributions are welcome! Here's how:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/AmazingFeature`)
3. **Commit** changes (`git commit -m 'Add some AmazingFeature'`)
4. **Push** to branch (`git push origin feature/AmazingFeature`)
5. **Open** a Pull Request

### Adding Models
Please ensure new models include:
- Accurate pricing from official sources
- Context window size
- Benchmark scores (if available)
- Proper provider attribution
- Release date
- **API information** (endpoint, authentication, rate limits, regional availability, documentation)
- **Status badges** (isNew for 2024-2025 releases, pricingUpdated with date)
- Model strengths and description
- Only **real production models** - no fake or placeholder entries

---

## üìù License

This project is open source and available under the [MIT License](LICENSE).

---

## üôè Acknowledgments

- **Pricing data** sourced from official provider documentation
- **Benchmark scores** from published research papers and official announcements
- **Icons** by [Lucide](https://lucide.dev/)
- **Framework** by [Next.js](https://nextjs.org/)
- **Styling** by [Tailwind CSS](https://tailwindcss.com/)

---

## üìß Contact & Support

- **Issues**: [GitHub Issues](https://github.com/your-repo/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-repo/discussions)
- **Updates**: Follow for latest model additions

---

## ‚ö†Ô∏è Disclaimer

**Pricing and Model Information**: Updated regularly but may not always reflect the latest changes. Please verify critical information with official provider documentation.

**Code Examples**: Provided for illustrative and educational purposes. Test thoroughly before using in production.

**Real-World Use Cases**: Illustrative examples based on common use patterns. Actual results may vary. Consult vendor documentation for verified case studies.

**Limitations & Best Practices**: General guidance based on common patterns. Always refer to official provider documentation for the most current information.

This tool is for informational purposes only.

---

Made with ‚ù§Ô∏è by Sriram Srinivasan

**Last Updated**: November 8, 2025

---

## üìä Database Statistics

**Models & Coverage:**
- **150 production models** across 39 providers
- **100% API coverage** - every model has complete API information
- **122 NEW models** - released in 2024-2025
- **81 models** with recent pricing updates
- **Latest additions**: o1, o3-mini, Codestral, Gemini 2.0 Flash, DeepSeek V3, Gemma 2 9B/27B

**Phase 1 Content (NEW):**
- **1,200+ code examples** (8 per model √ó 150 models)
- **600+ use case scenarios** (3-5 per model)
- **150 comprehensive limitation guides**
- **3 programming languages** supported (Python, JavaScript, cURL)
- **7 industry verticals** covered
